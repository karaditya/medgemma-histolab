{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetConnected": true,
   "language": "python",
   "sourceType": "notebook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# HistoLab — MedGemma Demo on Kaggle GPU\n",
    "\n",
    "Runs the fine-tuned MedGemma-4B histopathology classifier on a **free Kaggle T4 GPU**  \n",
    "and exposes it via a public Gradio link (valid ~72 hours).\n",
    "\n",
    "### Before running:\n",
    "1. Enable **GPU** in *Session options → Accelerator → GPU T4 x1*\n",
    "2. Enable **Internet** in *Session options → Internet → On*\n",
    "3. Add two **Kaggle Secrets** (the lock icon on the left sidebar):\n",
    "   - `HF_TOKEN` — your HuggingFace token (needs read access to `google/medgemma-4b-it`)\n",
    "   - `ADAPTER_REPO_ID` — e.g. `karadi97/medgemma-histolab-5k`\n",
    "4. Add the **demo image dataset**: click *Add data* (➕) → search `histolab-medgemma-demo-samples` → Add  \n",
    "   *(enables the BACH / CRC / PCAM quick-load buttons in the Gradio UI)*\n",
    "5. **Run All** and copy the `gradio.live` URL from the last cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "print('GPU:', result.stdout.strip() or 'NOT FOUND — enable GPU in Session options!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secrets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets from Kaggle Secrets (add them via the lock icon in the left sidebar)\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "os.environ['HF_TOKEN'] = secrets.get_secret('HF_TOKEN')\n",
    "os.environ['ADAPTER_REPO_ID'] = secrets.get_secret('ADAPTER_REPO_ID')\n",
    "\n",
    "print('HF_TOKEN set:        ', bool(os.environ.get('HF_TOKEN')))\n",
    "print('ADAPTER_REPO_ID set: ', os.environ.get('ADAPTER_REPO_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (takes ~2-3 min on first run)\n!pip install -q \\\n    'transformers>=4.49.0' \\\n    'accelerate>=0.25.0' \\\n    'bitsandbytes>=0.41.0' \\\n    'peft>=0.7.0' \\\n    'gradio>=5.0.0,<7.0.0' \\\n    'huggingface_hub>=0.19.0' \\\n    'Pillow>=10.0.0'\nprint('Dependencies installed.')\n\n# Show versions for debugging\nimport transformers, peft, torch\nprint(f'  transformers: {transformers.__version__}')\nprint(f'  peft:         {peft.__version__}')\nprint(f'  torch:        {torch.__version__}')\nprint(f'  CUDA:         {torch.version.cuda}')\nif torch.cuda.is_available():\n    print(f'  GPU:          {torch.cuda.get_device_name(0)}')\n    print(f'  bf16 native:  {torch.cuda.is_bf16_supported()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (or pull latest if already cloned)\n",
    "import os\n",
    "\n",
    "REPO_URL = 'https://github.com/karaditya/medgemma-histolab.git'\n",
    "REPO_DIR = '/kaggle/working/medgemma-histolab'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone --depth 1 {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull --ff-only origin main\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print('Working directory:', os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-pkg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the local histolab package\n",
    "!pip install -q -e . --no-deps\n",
    "print('histolab package installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "link-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link the demo image dataset into the path the app expects\n",
    "# Auto-detects the dataset from /kaggle/input/ (handles all mount formats)\n",
    "import pathlib, os\n",
    "\n",
    "data_dir = pathlib.Path('/kaggle/working/medgemma-histolab/data/datasets')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Search /kaggle/input/ recursively for a folder containing crc/bach/pcam\n",
    "# Kaggle mounts datasets at varying depths:\n",
    "#   /kaggle/input/<slug>/                          (classic)\n",
    "#   /kaggle/input/datasets/<user>/<slug>/          (new API)\n",
    "kaggle_input = pathlib.Path('/kaggle/input')\n",
    "dataset_root = None\n",
    "target_dirs = {'crc', 'bach', 'pcam'}\n",
    "\n",
    "for root, dirs, files in os.walk(str(kaggle_input)):\n",
    "    root_path = pathlib.Path(root)\n",
    "    if target_dirs & set(dirs):\n",
    "        dataset_root = root_path\n",
    "        break\n",
    "    # Don't recurse deeper than 4 levels\n",
    "    if len(root_path.relative_to(kaggle_input).parts) >= 4:\n",
    "        dirs.clear()\n",
    "\n",
    "if dataset_root is None:\n",
    "    print('WARNING: No demo dataset found under /kaggle/input/')\n",
    "    print('  Add it via: Add data (+) -> search histolab-medgemma-demo-samples -> Add')\n",
    "    # Debug: show what IS mounted\n",
    "    for p in sorted(kaggle_input.rglob('*')):\n",
    "        if p.is_dir() and len(p.relative_to(kaggle_input).parts) <= 3:\n",
    "            print(f'    {p}')\n",
    "else:\n",
    "    print(f'Found dataset at: {dataset_root}')\n",
    "    for ds in ['crc', 'bach', 'pcam']:\n",
    "        src = dataset_root / ds\n",
    "        dst = data_dir / ds\n",
    "        if dst.exists() or dst.is_symlink():\n",
    "            print(f'  already linked: {ds}')\n",
    "        elif src.exists():\n",
    "            dst.symlink_to(src)\n",
    "            n_imgs = sum(1 for _ in src.rglob('*') if _.is_file())\n",
    "            print(f'  linked {ds}: {n_imgs} files')\n",
    "        else:\n",
    "            print(f'  {ds}: not in dataset (skipping)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preload-model",
   "metadata": {},
   "outputs": [],
   "source": "# Pre-load the model so errors show here (not hidden behind Gradio UI)\n# This is the slow step: ~3-5 min on first run (downloads base model + merges adapter)\nimport sys, gc, shutil, torch\nsys.path.insert(0, REPO_DIR)\n\n# Force-clear any stale merged model cache from a previous run\n# (ensures the merge runs fresh with the latest code/adapter)\nadapter_merged = os.path.join(REPO_DIR, 'adapter', 'merged_model')\nif os.path.exists(adapter_merged):\n    print(f'Clearing stale merged model cache: {adapter_merged}')\n    shutil.rmtree(adapter_merged)\n\nimport app as histolab_app\n\nprint('Loading model...')\ntry:\n    histolab_app.wrapper.load()\n    print(f'Model loaded successfully!')\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated() / 1024**3\n        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        print(f'GPU memory: {alloc:.1f} / {total:.1f} GiB')\n\n    # Verify chat template is present\n    proc = histolab_app.wrapper.processor\n    has_template = getattr(proc.tokenizer, 'chat_template', None) is not None\n    print(f'Chat template loaded: {has_template}')\n    if not has_template:\n        print('WARNING: No chat template! Model will likely produce empty output.')\n\n    # Quick test inference to verify model works\n    print('\\n--- Test inference ---')\n    from PIL import Image\n    test_img = Image.new('RGB', (224, 224), color=(180, 120, 160))\n    test_result = histolab_app.wrapper.analyze_patch(\n        image=test_img,\n        prompt='What type of tissue is this? Answer briefly.',\n        max_new_tokens=32\n    )\n    raw = test_result.get('raw_response', '')\n    print(f'Test output ({len(raw)} chars): {repr(raw[:200])}')\n    if not raw.strip():\n        print('WARNING: Model produced empty output! Check adapter merge and chat template.')\n    else:\n        print('Model is generating text correctly.')\n\nexcept Exception as e:\n    print(f'ERROR loading model: {e}')\n    import traceback; traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio (model already loaded above)\n",
    "histolab_app.app_instance.model_loaded = True\n",
    "\n",
    "histolab_app.demo.launch(\n",
    "    share=True,\n",
    "    show_api=False,\n",
    "    quiet=False,\n",
    ")\n"
   ]
  }
 ]
}